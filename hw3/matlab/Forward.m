function [out, act_h, act_a] = Forward(W, b, X)
% [OUT, act_h, act_a] = Forward(W, b, X) performs forward propogation on the
% input data 'X' uisng the network defined by weights and biases 'W' and 'b'
% (as generated by InitializeNetwork(..)).
%
% This function should return the final softmax output layer activations in OUT,
% as well as the hidden layer post activations in 'act_h', and the hidden layer
% pre activations in 'act_a'.

if numel(X) ~= size(X, 1)
    X = reshape(X, [numel(X), 1]);
end

num_layers = length(W);
act_a = cell(1, num_layers);
act_h = cell(1, num_layers);

act_a{1} = W{1} * X + b{1};
act_h{1} = 1 ./ (1 + exp(-act_a{1}));
act_h{1}(isnan(act_h{1})) = 0;

for i = 2:num_layers
    act_a{i} = W{i} * act_h{i-1} + b{i};
    
    if i ~= num_layers
        act_h{i} = 1 ./ (1 + exp(-act_a{i}));
        act_h{i}(isnan(act_h{i})) = 0;
    else
        exps = exp(act_a{i});
        sm = sum(exps);
        act_h{i} = exps ./ sm;
        act_h{i}(isnan(act_h{i})) = 0;
        out = act_h{i};
    end
end
end
